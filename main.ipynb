{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0",
      "metadata": {
        "id": "0"
      },
      "source": [
        "### ***                 CH·ª¶ ƒê·ªÄ:  ·ª®NG D·ª§NG M√î H√åNH H·ªåC M√ÅY V√ÄO D·ª∞ ƒêO√ÅN S·ªê ƒêI·ªÜN S·ª¨ D·ª§NG TRONG TH√ÅNG***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1",
      "metadata": {
        "id": "1"
      },
      "source": [
        "# C√°ch s·ª≠ d·ª•ng tr√™n Google Colab:\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**B∆∞·ªõc 1: C√†i ƒë·∫∑t kagglehub (ch·∫°y cell ƒë·∫ßu ti√™n)**\n",
        "\n",
        "**B∆∞·ªõc 2: X√°c th·ª±c Kaggle (·ªû ƒë√¢y dataset ta ch·ªçn electric-power-consumption-data-set l√† public n√™n kh√¥ng c·∫ßn x√°c th·ª±c)**\n",
        "\n",
        "**B∆∞·ªõc 3: Ch·∫°y c√°c cell theo th·ª© t·ª±**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2",
      "metadata": {
        "id": "2"
      },
      "outputs": [],
      "source": [
        "#C√†i ƒë·∫∑t v√† import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
        "!pip install kagglehub[pandas-datasets]\n",
        "# C√†i ƒë·∫∑t optuna (n·∫øu ch∆∞a c√≥)\n",
        "!pip install optuna\n",
        "\n",
        "# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt cho qu√° tr√¨nh t·ªëi ∆∞u\n",
        "import optuna\n",
        "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import kagglehub\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "import xgboost as xgb\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. TI·ªÄN X·ª¨ L√ù D·ªÆ LI·ªÜU (PREPROCESSING)**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "*1.1. T·∫£i v√† L√†m s·∫°ch d·ªØ li·ªáu*\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "6DhrjecTJOQK"
      },
      "id": "6DhrjecTJOQK"
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.1 T·∫£i d·ªØ li·ªáu\n",
        "path = kagglehub.dataset_download(\"uciml/electric-power-consumption-data-set\")\n",
        "df = pd.read_csv(f\"{path}/household_power_consumption.txt\", sep=';',\n",
        "                 parse_dates={'datetime': ['Date', 'Time']},\n",
        "                 infer_datetime_format=True,\n",
        "                 low_memory=False, na_values=['?'])\n",
        "df.set_index('datetime', inplace=True)\n",
        "\n",
        "# 1.2 Gom nh√≥m theo ng√†y (Resampling)\n",
        "df_daily = df['Global_active_power'].resample('D').sum().to_frame()\n",
        "\n",
        "# 1.3 X·ª≠ l√Ω d·ªØ li·ªáu thi·∫øu (Interpolation)\n",
        "df_daily['Global_active_power'] = df_daily['Global_active_power'].interpolate(method='time')\n",
        "\n",
        "# 1.4 X·ª≠ l√Ω gi√° tr·ªã ngo·∫°i lai (Outlier Handling) - GI√öP GI·∫¢M SAI S·ªê\n",
        "# Gi·ªõi h·∫°n gi√° tr·ªã ·ªü m·ª©c ph√¢n v·ªã 99% ƒë·ªÉ lo·∫°i b·ªè c√°c ƒë·ªânh nhi·ªÖu b·∫•t th∆∞·ªùng\n",
        "upper_limit = df_daily['Global_active_power'].quantile(0.99)\n",
        "df_daily['Global_active_power'] = df_daily['Global_active_power'].clip(upper=upper_limit)\n",
        "\n",
        "print(f\"‚úì Ho√†n th√†nh ti·ªÅn x·ª≠ l√Ω. S·ªë d√≤ng: {len(df_daily)}\")"
      ],
      "metadata": {
        "id": "WZvz83_SJbly"
      },
      "id": "WZvz83_SJbly",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**2. KH√ÅM PH√Å D·ªÆ LI·ªÜU TR·ª∞C QUAN (EDA)**\n",
        "---\n",
        "2.1. Ph√¢n b·ªï Bi·∫øn m·ª•c ti√™u\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dx0kxp8A0mVV"
      },
      "id": "dx0kxp8A0mVV"
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "# Bi·ªÉu ƒë·ªì 1: Ph√¢n b·ªï ti√™u th·ª• ƒëi·ªán\n",
        "sns.histplot(df_daily['Global_active_power'], kde=True, ax=axes[0], color='teal')\n",
        "axes[0].set_title('Ph√¢n b·ªï m·ª©c ti√™u th·ª• ƒëi·ªán h√†ng ng√†y (kWh)')\n",
        "\n",
        "# Bi·ªÉu ƒë·ªì 2: Xu h∆∞·ªõng d√†i h·∫°n (Rolling Mean)\n",
        "axes[1].plot(df_daily['Global_active_power'], alpha=0.3, label='H√†ng ng√†y')\n",
        "axes[1].plot(df_daily['Global_active_power'].rolling(30).mean(), color='red', label='Trung b√¨nh th√°ng')\n",
        "axes[1].set_title('Xu h∆∞·ªõng ti√™u th·ª• ƒëi·ªán qua c√°c nƒÉm')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UqyNgeYG0xp1"
      },
      "id": "UqyNgeYG0xp1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "## **3.FEATURE ENGINEERING (K·ª∏ THU·∫¨T ƒê·∫∂C TR∆ØNG)**\n",
        "\n",
        "---\n",
        "\n",
        "3.1 Bi·∫øn ƒë·ªïi chu·ªói th·ªùi gian th√†nh b√†i to√°n H·ªçc m√°y c√≥ gi√°m s√°t."
      ],
      "metadata": {
        "id": "xviMQQEtJhgq"
      },
      "id": "xviMQQEtJhgq"
    },
    {
      "cell_type": "code",
      "source": [
        "def create_advanced_features(df):\n",
        "    df = df.copy()\n",
        "    # ƒê·∫∑c tr∆∞ng th·ªùi gian\n",
        "    df['dayofweek'] = df.index.dayofweek\n",
        "    df['month'] = df.index.month\n",
        "    df['is_weekend'] = df['dayofweek'].isin([5, 6]).astype(int)\n",
        "\n",
        "    # M√£ h√≥a chu k·ª≥ (Cyclical Encoding)\n",
        "    df['month_sin'] = np.sin(2 * np.pi * df['month']/12)\n",
        "    df['month_cos'] = np.cos(2 * np.pi * df['month']/12)\n",
        "\n",
        "    # ƒê·∫∑c tr∆∞ng l·ªãch s·ª≠ (Lags) - Quan tr·ªçng nh·∫•t trong h·ªçc c√≥ gi√°m s√°t\n",
        "    df['lag_1'] = df['Global_active_power'].shift(1)\n",
        "    df['lag_7'] = df['Global_active_power'].shift(7)\n",
        "\n",
        "    # ƒê·∫∑c tr∆∞ng c·ª≠a s·ªï tr∆∞·ª£t (Rolling)\n",
        "    df['rolling_mean_7'] = df['Global_active_power'].rolling(window=7).mean().shift(1)\n",
        "    df['rolling_std_7'] = df['Global_active_power'].rolling(window=7).std().shift(1)\n",
        "\n",
        "    return df.dropna()\n",
        "\n",
        "data = create_advanced_features(df_daily)\n",
        "\n",
        "# V·∫Ω ma tr·∫≠n t∆∞∆°ng quan ƒë·ªÉ xem bi·∫øn n√†o ·∫£nh h∆∞·ªüng nh·∫•t\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(data.corr(), annot=True, fmt='.2f', cmap='coolwarm')\n",
        "plt.title('Ma tr·∫≠n t∆∞∆°ng quan gi·ªØa c√°c ƒë·∫∑c tr∆∞ng')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ohhEIMoQJm_p"
      },
      "id": "ohhEIMoQJm_p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. HU·∫§N LUY·ªÜN V√Ä D·ª∞ B√ÅO CU·ªêN CHI·∫æU (RECURSIVE FORECASTING)**\n",
        "\n",
        "---\n",
        "4.1. Chia d·ªØ li·ªáu v√† Chu·∫©n h√≥a\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RgZhhTUrKA3I"
      },
      "id": "RgZhhTUrKA3I"
    },
    {
      "cell_type": "code",
      "source": [
        "data = create_advanced_features(df_daily)\n",
        "train_size = int(len(data) * 0.8)\n",
        "train_data, test_data = data.iloc[:train_size], data.iloc[train_size:]\n",
        "\n",
        "X_train = train_data.drop('Global_active_power', axis=1)\n",
        "y_train = train_data['Global_active_power']\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)"
      ],
      "metadata": {
        "id": "dUz5aXsHKYhS"
      },
      "id": "dUz5aXsHKYhS",
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "4.2. Workflow D·ª± b√°o cu·ªën chi·∫øu\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "oTvyhMHCKn2y"
      },
      "id": "oTvyhMHCKn2y"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def recursive_forecast(model, scaler, history_df, steps=30):\n",
        "    \"\"\"\n",
        "    D·ª± b√°o cu·ªën chi·∫øu 30 ng√†y\n",
        "    \"\"\"\n",
        "    current_history = history_df.copy()\n",
        "    forecasts = []\n",
        "\n",
        "    for _ in range(steps):\n",
        "        # 1. T·∫°o feature t·ª´ d·ªØ li·ªáu l·ªãch s·ª≠ hi·ªán t·∫°i\n",
        "        features = create_advanced_features(current_history)\n",
        "        X_next = features.iloc[-1:].drop('Global_active_power', axis=1)\n",
        "\n",
        "        # 2. Chu·∫©n h√≥a v√† d·ª± b√°o\n",
        "        X_next_scaled = scaler.transform(X_next)\n",
        "        y_pred = model.predict(X_next_scaled)[0]\n",
        "\n",
        "        # ƒê·∫£m b·∫£o k·∫øt qu·∫£ kh√¥ng √¢m\n",
        "        y_pred = max(0, y_pred)\n",
        "        forecasts.append(y_pred)\n",
        "\n",
        "        # 3. C·∫≠p nh·∫≠t l·ªãch s·ª≠: Th√™m d√≤ng d·ª± b√°o m·ªõi v√†o ƒë·ªÉ t√≠nh feature cho ng√†y k·∫ø ti·∫øp\n",
        "        next_date = current_history.index[-1] + pd.Timedelta(days=1)\n",
        "        new_row = pd.DataFrame({'Global_active_power': [y_pred]}, index=[next_date])\n",
        "        current_history = pd.concat([current_history, new_row])\n",
        "\n",
        "    return np.array(forecasts) # Tr·∫£ v·ªÅ Numpy Array ƒë·ªÉ tr√°nh l·ªói .sum()"
      ],
      "metadata": {
        "id": "TB_jTxYLKuKi"
      },
      "id": "TB_jTxYLKuKi",
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. TH·ª∞C NGHI·ªÜM V√Ä T·ªêI ∆ØU T·ª™NG M√î H√åNH**\n",
        "\n",
        "---\n",
        "\n",
        "5.1. H√†m ƒë√°nh gi√° m√¥ h√¨nh tr√™n Recursive Forecast\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "wZ9B6r39MEzT"
      },
      "id": "wZ9B6r39MEzT"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dVwUuMFLiHF-"
      },
      "id": "dVwUuMFLiHF-"
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_recursive_forecast(model, scaler, df_daily, train_size, forecast_horizon=30):\n",
        "    \"\"\"\n",
        "    H√†m m·ª•c ti√™u ƒë·ªÉ Optuna t·ªëi ∆∞u (T√≠nh sai s·ªë t·ªïng h√≥a ƒë∆°n)\n",
        "    \"\"\"\n",
        "    # L·∫•y 35 ng√†y tr∆∞·ªõc t·∫≠p test ƒë·ªÉ l√†m ƒë·∫∑c tr∆∞ng (lag)\n",
        "    history_df = df_daily.iloc[train_size - 35 : train_size]\n",
        "\n",
        "    # Gi√° tr·ªã th·ª±c t·∫ø 30 ng√†y\n",
        "    actual = df_daily.iloc[train_size : train_size + forecast_horizon]['Global_active_power']\n",
        "\n",
        "    # D·ª± b√°o cu·ªën chi·∫øu\n",
        "    predictions = recursive_forecast(model, scaler, history_df, steps=forecast_horizon)\n",
        "\n",
        "    # ƒê·∫£m b·∫£o predictions l√† numpy array\n",
        "    predictions = np.array(predictions)\n",
        "\n",
        "    # T√≠nh sai s·ªë t·ªïng h√≥a ƒë∆°n (%)\n",
        "    actual_sum = actual.sum()\n",
        "    if actual_sum == 0: return 100.0 # Tr√°nh l·ªói chia cho 0\n",
        "\n",
        "    total_error_pct = abs(actual_sum - predictions.sum()) / actual_sum * 100\n",
        "\n",
        "    return total_error_pct"
      ],
      "metadata": {
        "id": "t1ZksuW0MMj6"
      },
      "id": "t1ZksuW0MMj6",
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "5.2 T·ªêI ∆ØU LINEAR REGRESSION\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "GQVYbgX1jbuv"
      },
      "id": "GQVYbgX1jbuv"
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_linear_regression(df_daily, train_size, n_trials=30):\n",
        "    \"\"\"\n",
        "    T·ªëi ∆∞u Linear Regression v·ªõi regularization\n",
        "\n",
        "    Tham s·ªë t·ªëi ∆∞u:\n",
        "    - type: 'none' (OLS), 'ridge' (L2), 'lasso' (L1)\n",
        "    - alpha: H·ªá s·ªë regularization (n·∫øu d√πng Ridge/Lasso)\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"PH·∫¶N 2.1: T·ªêI ∆ØU LINEAR REGRESSION\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Chu·∫©n b·ªã d·ªØ li·ªáu\n",
        "    data = create_advanced_features(df_daily)\n",
        "    train_data = data.iloc[:train_size]\n",
        "    X_train = train_data.drop('Global_active_power', axis=1)\n",
        "    y_train = train_data['Global_active_power']\n",
        "    scaler = MinMaxScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "    def objective(trial):\n",
        "        # Ch·ªçn lo·∫°i model\n",
        "        reg_type = trial.suggest_categorical('type', ['ridge', 'lasso', 'none'])\n",
        "\n",
        "        if reg_type == 'ridge':\n",
        "            alpha = trial.suggest_float('alpha', 0.001, 100, log=True)\n",
        "            model = Ridge(alpha=alpha)\n",
        "        elif reg_type == 'lasso':\n",
        "            alpha = trial.suggest_float('alpha', 0.001, 100, log=True)\n",
        "            model = Lasso(alpha=alpha, max_iter=5000)\n",
        "        else:\n",
        "            model = LinearRegression()\n",
        "\n",
        "        # Train\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "\n",
        "        # ƒê√°nh gi√°\n",
        "        error = evaluate_recursive_forecast(model, scaler, df_daily, train_size)\n",
        "        return error\n",
        "\n",
        "    # T·ªëi ∆∞u v·ªõi Optuna\n",
        "    study = optuna.create_study(direction='minimize')\n",
        "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
        "\n",
        "    # In k·∫øt qu·∫£\n",
        "    print(f\"\\n‚úì Ho√†n th√†nh t·ªëi ∆∞u Linear Regression\")\n",
        "    print(f\"Best Total Error: {study.best_value:.3f}%\")\n",
        "    print(f\"Best params: {study.best_params}\")\n",
        "\n",
        "    return study"
      ],
      "metadata": {
        "id": "ZIIFBJxTjjN2"
      },
      "id": "ZIIFBJxTjjN2",
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "5.3 T·ªêI ∆ØU RANDOM FOREST\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "sT2HqJJajxcu"
      },
      "id": "sT2HqJJajxcu"
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_random_forest(df_daily, train_size, n_trials=100):\n",
        "    \"\"\"\n",
        "    T·ªëi ∆∞u Random Forest v·ªõi Optuna\n",
        "\n",
        "    Search space:\n",
        "    - n_estimators: [50, 300] - S·ªë c√¢y trong r·ª´ng\n",
        "    - max_depth: [2, 10] - ƒê·ªô s√¢u c√¢y (quan tr·ªçng cho recursive forecast)\n",
        "    - min_samples_split: [2, 20] - S·ªë samples t·ªëi thi·ªÉu ƒë·ªÉ split\n",
        "    - min_samples_leaf: [1, 10] - S·ªë samples t·ªëi thi·ªÉu ·ªü l√°\n",
        "    - max_features: ['sqrt', 'log2', None] - S·ªë features m·ªói split\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"PH·∫¶N 2.2: T·ªêI ∆ØU RANDOM FOREST\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Chu·∫©n b·ªã d·ªØ li·ªáu\n",
        "    data = create_advanced_features(df_daily)\n",
        "    train_data = data.iloc[:train_size]\n",
        "    X_train = train_data.drop('Global_active_power', axis=1)\n",
        "    y_train = train_data['Global_active_power']\n",
        "    scaler = MinMaxScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "    def objective(trial):\n",
        "        params = {\n",
        "            'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
        "            'max_depth': trial.suggest_int('max_depth', 2, 10),\n",
        "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
        "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
        "            'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        model = RandomForestRegressor(**params)\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        error = evaluate_recursive_forecast(model, scaler, df_daily, train_size)\n",
        "\n",
        "        return error\n",
        "\n",
        "    study = optuna.create_study(direction='minimize')\n",
        "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
        "\n",
        "    print(f\"\\n‚úì Ho√†n th√†nh t·ªëi ∆∞u Random Forest\")\n",
        "    print(f\"Best Total Error: {study.best_value:.3f}%\")\n",
        "    print(f\"Best params: {study.best_params}\")\n",
        "\n",
        "    return study\n"
      ],
      "metadata": {
        "id": "4K6pPSmej0a_"
      },
      "id": "4K6pPSmej0a_",
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "5.4 T·ªêI ∆ØU GRADIENT BOOSTING\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "srGb9mg8j43P"
      },
      "id": "srGb9mg8j43P"
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_gradient_boosting(df_daily, train_size, n_trials=100):\n",
        "    \"\"\"\n",
        "    T·ªëi ∆∞u Gradient Boosting v·ªõi Optuna\n",
        "\n",
        "    Search space:\n",
        "    - n_estimators: [50, 400]\n",
        "    - learning_rate: [0.001, 0.05] - LOG SCALE (quan tr·ªçng!)\n",
        "    - max_depth: [2, 6] - C√¢y n√¥ng h∆°n RF\n",
        "    - subsample: [0.6, 1.0] - Regularization\n",
        "    - min_samples_split/leaf: Ki·ªÉm so√°t overfitting\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"PH·∫¶N 2.3: T·ªêI ∆ØU GRADIENT BOOSTING\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Chu·∫©n b·ªã d·ªØ li·ªáu\n",
        "    data = create_advanced_features(df_daily)\n",
        "    train_data = data.iloc[:train_size]\n",
        "    X_train = train_data.drop('Global_active_power', axis=1)\n",
        "    y_train = train_data['Global_active_power']\n",
        "    scaler = MinMaxScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "    def objective(trial):\n",
        "        params = {\n",
        "            'n_estimators': trial.suggest_int('n_estimators', 50, 400),\n",
        "            'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.05, log=True),\n",
        "            'max_depth': trial.suggest_int('max_depth', 2, 6),\n",
        "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
        "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
        "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        model = GradientBoostingRegressor(**params)\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        error = evaluate_recursive_forecast(model, scaler, df_daily, train_size)\n",
        "\n",
        "        return error\n",
        "\n",
        "    study = optuna.create_study(direction='minimize')\n",
        "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
        "\n",
        "    print(f\"\\n‚úì Ho√†n th√†nh t·ªëi ∆∞u Gradient Boosting\")\n",
        "    print(f\"Best Total Error: {study.best_value:.3f}%\")\n",
        "    print(f\"Best params: {study.best_params}\")\n",
        "\n",
        "    return study"
      ],
      "metadata": {
        "id": "zwy627P0j8SW"
      },
      "id": "zwy627P0j8SW",
      "execution_count": 257,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "5.5 T·ªêI ∆ØU XGBOOST\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "98OiFxoEkBN-"
      },
      "id": "98OiFxoEkBN-"
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_xgboost(df_daily, train_size, n_trials=100):\n",
        "    \"\"\"\n",
        "    T·ªëi ∆∞u XGBoost v·ªõi Optuna\n",
        "\n",
        "    Search space (r·ªông h∆°n GB):\n",
        "    - n_estimators: [50, 400]\n",
        "    - learning_rate: [0.001, 0.05] - LOG SCALE\n",
        "    - max_depth: [2, 8] - S√¢u h∆°n GB m·ªôt ch√∫t\n",
        "    - subsample: [0.6, 1.0]\n",
        "    - colsample_bytree: [0.6, 1.0] - Unique cho XGBoost\n",
        "    - min_child_weight: [1, 10]\n",
        "    - gamma: [0, 0.5] - Minimum loss reduction\n",
        "    - reg_alpha, reg_lambda: [0, 1] - L1/L2 regularization\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"PH·∫¶N 2.4: T·ªêI ∆ØU XGBOOST\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Chu·∫©n b·ªã d·ªØ li·ªáu\n",
        "    data = create_advanced_features(df_daily)\n",
        "    train_data = data.iloc[:train_size]\n",
        "    X_train = train_data.drop('Global_active_power', axis=1)\n",
        "    y_train = train_data['Global_active_power']\n",
        "    scaler = MinMaxScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "    def objective(trial):\n",
        "        params = {\n",
        "            'n_estimators': trial.suggest_int('n_estimators', 50, 400),\n",
        "            'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.05, log=True),\n",
        "            'max_depth': trial.suggest_int('max_depth', 2, 8),\n",
        "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
        "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
        "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
        "            'gamma': trial.suggest_float('gamma', 0, 0.5),\n",
        "            'reg_alpha': trial.suggest_float('reg_alpha', 0, 1),\n",
        "            'reg_lambda': trial.suggest_float('reg_lambda', 0, 1),\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        model = xgb.XGBRegressor(**params)\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        error = evaluate_recursive_forecast(model, scaler, df_daily, train_size)\n",
        "\n",
        "        return error\n",
        "\n",
        "    study = optuna.create_study(direction='minimize')\n",
        "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
        "\n",
        "    print(f\"\\n‚úì Ho√†n th√†nh t·ªëi ∆∞u XGBoost\")\n",
        "    print(f\"Best Total Error: {study.best_value:.3f}%\")\n",
        "    print(f\"Best params: {study.best_params}\")\n",
        "\n",
        "    return study"
      ],
      "metadata": {
        "id": "AJ-wctoUkOzu"
      },
      "id": "AJ-wctoUkOzu",
      "execution_count": 258,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. SO S√ÅNH V√Ä ƒê√ÅNH GI√Å K·∫æT QU·∫¢**\n",
        "\n",
        "---\n",
        "\n",
        "6.1. Th·ª±c hi·ªán t·ªëi ∆∞u c√°c m√¥ h√¨nh v√† so s√°nh\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "CXLe97bWNEji"
      },
      "id": "CXLe97bWNEji"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "def compare_optimized_models(studies_dict, df_daily, train_size):\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"B·∫ÆT ƒê·∫¶U SO S√ÅNH C√ÅC M√î H√åNH SAU KHI T·ªêI ∆ØU\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Chu·∫©n b·ªã d·ªØ li·ªáu hu·∫•n luy·ªán\n",
        "    data = create_advanced_features(df_daily)\n",
        "    train_data = data.iloc[:train_size]\n",
        "    X_train = train_data.drop('Global_active_power', axis=1)\n",
        "    y_train = train_data['Global_active_power']\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "    history_df = df_daily.iloc[train_size - 35 : train_size]\n",
        "    actual_30d = df_daily.iloc[train_size : train_size + 30]['Global_active_power']\n",
        "\n",
        "    results = {}\n",
        "    models_trained = {}\n",
        "\n",
        "    for name, study in studies_dict.items():\n",
        "        print(f\"\\n[+] ƒêang hu·∫•n luy·ªán l·∫°i {name}...\")\n",
        "        bp = study.best_params\n",
        "\n",
        "        # Kh·ªüi t·∫°o model v·ªõi best_params\n",
        "        if name == 'Linear Regression':\n",
        "            model = Ridge(alpha=bp['alpha']) if bp.get('type') == 'ridge' else \\\n",
        "                    Lasso(alpha=bp['alpha']) if bp.get('type') == 'lasso' else LinearRegression()\n",
        "        elif name == 'Random Forest':\n",
        "            model = RandomForestRegressor(n_estimators=bp['n_estimators'], max_depth=bp['max_depth'], random_state=42)\n",
        "        elif name == 'Gradient Boosting':\n",
        "            model = GradientBoostingRegressor(**bp, random_state=42)\n",
        "        elif name == 'XGBoost':\n",
        "            import xgboost as xgb\n",
        "            model = xgb.XGBRegressor(**bp, random_state=42)\n",
        "\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        models_trained[name] = model\n",
        "\n",
        "        # D·ª± b√°o 30 ng√†y\n",
        "        preds = recursive_forecast(model, scaler, history_df, steps=30)\n",
        "\n",
        "        # T√≠nh to√°n c√°c ch·ªâ s·ªë\n",
        "        rmse = np.sqrt(mean_squared_error(actual_30d, preds))\n",
        "        mae = mean_absolute_error(actual_30d, preds)\n",
        "        mape = np.mean(np.abs((actual_30d - preds) / actual_30d)) * 100\n",
        "        r2 = r2_score(actual_30d, preds)\n",
        "        total_err = abs(actual_30d.sum() - preds.sum()) / actual_30d.sum() * 100\n",
        "\n",
        "        results[name] = {\n",
        "            'RMSE': rmse, 'MAE': mae, 'MAPE': mape, 'R¬≤': r2,\n",
        "            'Total_Error_Pct': total_err, 'predictions': preds, 'Forecasts': preds\n",
        "        }\n",
        "        print(f\"    ‚úì Ho√†n th√†nh. Sai s·ªë t·ªïng: {total_err:.2f}%\")\n",
        "\n",
        "    return results, models_trained, actual_30d\n",
        "\n",
        "# Call optimization functions to populate studies_dict before using it\n",
        "# Using fewer trials (n_trials=10) for demonstration purposes to speed up execution.\n",
        "# In a full run, consider increasing n_trials for better optimization.\n",
        "print(\"\\n===== B·∫ÆT ƒê·∫¶U T·ªêI ∆ØU C√ÅC M√î H√åNH ====\")\n",
        "linear_study = optimize_linear_regression(df_daily, train_size, n_trials=10)\n",
        "rf_study = optimize_random_forest(df_daily, train_size, n_trials=10)\n",
        "gb_study = optimize_gradient_boosting(df_daily, train_size, n_trials=10)\n",
        "xgb_study = optimize_xgboost(df_daily, train_size, n_trials=10)\n",
        "print(\"===== HO√ÄN TH√ÄNH T·ªêI ∆ØU C√ÅC M√î H√åNH ====\")\n",
        "\n",
        "studies_dict = {\n",
        "    'Linear Regression': linear_study,\n",
        "    'Random Forest': rf_study,\n",
        "    'Gradient Boosting': gb_study,\n",
        "    'XGBoost': xgb_study\n",
        "}\n",
        "\n",
        "results, models_trained, actual_30d = compare_optimized_models(studies_dict, df_daily, train_size)\n"
      ],
      "metadata": {
        "id": "IzYS5MbzNLQ6"
      },
      "id": "IzYS5MbzNLQ6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "6.2. In b·∫£ng t·ªïng h·ª£p k·∫øt qu·∫£ chi ti·∫øt\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aqpTa60OOCEM"
      },
      "id": "aqpTa60OOCEM"
    },
    {
      "cell_type": "code",
      "source": [
        "def print_detailed_results(results, studies_dict):\n",
        "    \"\"\"\n",
        "    In b·∫£ng t·ªïng h·ª£p k·∫øt qu·∫£ chi ti·∫øt\n",
        "\n",
        "    ‚Üí ƒê∆∞a v√†o ph·∫ßn \"B·∫£ng k·∫øt qu·∫£\" c·ªßa b√°o c√°o\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" \"*20 + \"B·∫¢NG T·ªîNG H·ª¢P K·∫æT QU·∫¢\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # T·∫°o DataFrame t·ªïng h·ª£p\n",
        "    summary_data = []\n",
        "    for name, data in results.items():\n",
        "        summary_data.append({\n",
        "            'M√¥ h√¨nh': name,\n",
        "            'RMSE (kWh)': f\"{data['RMSE']:.2f}\",\n",
        "            'MAE (kWh)': f\"{data['MAE']:.2f}\",\n",
        "            'MAPE (%)': f\"{data['MAPE']:.2f}\",\n",
        "            'R¬≤': f\"{data['R¬≤']:.4f}\",\n",
        "            'Sai s·ªë t·ªïng (%)': f\"{data['Total_Error_Pct']:.2f}\"\n",
        "        })\n",
        "\n",
        "    df_summary = pd.DataFrame(summary_data).sort_values('Sai s·ªë t·ªïng (%)')\n",
        "    print(\"\\n\" + df_summary.to_string(index=False))\n",
        "\n",
        "    # In tham s·ªë t·ªëi ∆∞u\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"THAM S·ªê T·ªêI ∆ØU CHO T·ª™NG M√î H√åNH:\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    for name, study in studies_dict.items():\n",
        "        print(f\"\\n„Äê{name}„Äë\")\n",
        "        print(\"-\" * 60)\n",
        "        for key, value in study.best_params.items():\n",
        "            if isinstance(value, float):\n",
        "                print(f\"  {key:25s}: {value:.6f}\")\n",
        "            else:\n",
        "                print(f\"  {key:25s}: {value}\")\n",
        "        print(f\"  {'‚Üí Total Error':25s}: {study.best_value:.3f}%\")\n",
        "\n",
        "    # X√°c ƒë·ªãnh winner\n",
        "    best_model = df_summary.iloc[0]['M√¥ h√¨nh']\n",
        "    best_error = float(df_summary.iloc[0]['Sai s·ªë t·ªïng (%)'].replace('%', ''))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"üèÜ M√î H√åNH T·ªêT NH·∫§T: {best_model}\")\n",
        "    print(f\"   Sai s·ªë h√≥a ƒë∆°n th√°ng: {best_error:.2f}%\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "print_detailed_results(results, studies_dict)"
      ],
      "metadata": {
        "id": "D3UDOHAfOJ_r"
      },
      "id": "D3UDOHAfOJ_r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "6.3 T·∫°o c√°c bi·ªÉu ƒë·ªì so s√°nh\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "7W8wUDvnl3hY"
      },
      "id": "7W8wUDvnl3hY"
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_comparison(results, actual_30d):\n",
        "    \"\"\"Bi·ªÉu ƒë·ªì so s√°nh Total Error\"\"\"\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "    names = list(results.keys())\n",
        "    errors = [results[name]['Total_Error_Pct'] for name in names]\n",
        "    colors = ['green' if e < 1 else 'orange' if e < 5 else 'red' for e in errors]\n",
        "\n",
        "    bars = ax.barh(names, errors, color=colors, alpha=0.7, edgecolor='black')\n",
        "    ax.set_xlabel('Sai s·ªë ph·∫ßn trƒÉm (%)', fontsize=12, fontweight='bold')\n",
        "    ax.set_title('SO S√ÅNH SAI S·ªê H√ìA ƒê∆†N TH√ÅNG\\n(Sau t·ªëi ∆∞u hyperparameters)',\n",
        "                 fontsize=14, fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "    # Th√™m gi√° tr·ªã\n",
        "    for bar, error in zip(bars, errors):\n",
        "        ax.text(error + 0.2, bar.get_y() + bar.get_height()/2,\n",
        "                f'{error:.2f}%', va='center', fontweight='bold', fontsize=11)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def visualize_optimization_history(studies_dict):\n",
        "    \"\"\"Bi·ªÉu ƒë·ªì optimization history cho t·ª´ng m√¥ h√¨nh\"\"\"\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for idx, (name, study) in enumerate(studies_dict.items()):\n",
        "        trials_df = study.trials_dataframe()\n",
        "\n",
        "        axes[idx].plot(trials_df['number'], trials_df['value'],\n",
        "                      'o-', alpha=0.5, markersize=3)\n",
        "        axes[idx].axhline(study.best_value, color='red', linestyle='--',\n",
        "                         linewidth=2, label=f'Best: {study.best_value:.2f}%')\n",
        "        axes[idx].set_xlabel('Trial s·ªë', fontsize=10)\n",
        "        axes[idx].set_ylabel('Total Error (%)', fontsize=10)\n",
        "        axes[idx].set_title(f'{name}\\nQu√° tr√¨nh t·ªëi ∆∞u', fontsize=11, fontweight='bold')\n",
        "        axes[idx].legend()\n",
        "        axes[idx].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def visualize_forecasts(results, actual_30d):\n",
        "    \"\"\"\n",
        "    Bi·ªÉu ƒë·ªì d·ª± b√°o chi ti·∫øt 30 ng√†y: Th·ª±c t·∫ø vs C√°c m√¥ h√¨nh t·ªëi ∆∞u\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(16, 7))\n",
        "\n",
        "    # 1. V·∫Ω d·ªØ li·ªáu th·ª±c t·∫ø (L√†m n·ªïi b·∫≠t b·∫±ng m√†u ƒëen v√† ƒëi·ªÉm tr√≤n)\n",
        "    days = np.arange(1, 31)\n",
        "    ax.plot(days, actual_30d.values, 'o-', color='black', linewidth=3,\n",
        "            label='TH·ª∞C T·∫æ (Actual)', markersize=6, zorder=5)\n",
        "\n",
        "    # 2. V·∫Ω ƒë∆∞·ªùng d·ª± b√°o cho t·ª´ng m√¥ h√¨nh trong results\n",
        "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'] # M√†u s·∫Øc ƒë·∫∑c tr∆∞ng cho 4 m√¥ h√¨nh\n",
        "    linestyles = ['--', '--', '--', '--']\n",
        "\n",
        "    for idx, (name, data) in enumerate(results.items()):\n",
        "        # L·∫•y m·∫£ng d·ª± b√°o t·ª´ dictionary k·∫øt qu·∫£\n",
        "        preds = data['Forecasts']\n",
        "        ax.plot(days, preds, linestyle=linestyles[idx], color=colors[idx],\n",
        "                linewidth=2, label=f'D·ª± b√°o {name}', alpha=0.8)\n",
        "\n",
        "    # 3. Trang tr√≠ bi·ªÉu ƒë·ªì chuy√™n nghi·ªáp\n",
        "    ax.set_title('SO S√ÅNH CHI TI·∫æT D·ª∞ B√ÅO 30 NG√ÄY TI·∫æP THEO\\n(K·∫øt qu·∫£ sau khi t·ªëi ∆∞u Hyperparameters)',\n",
        "                 fontsize=15, fontweight='bold', pad=20)\n",
        "    ax.set_xlabel('Ng√†y trong chu k·ª≥ d·ª± b√°o (1-30)', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('S·ªë ƒëi·ªán ti√™u th·ª• (kWh)', fontsize=12, fontweight='bold')\n",
        "\n",
        "    # C·∫•u h√¨nh tr·ª•c X hi·ªÉn th·ªã ƒë·ªß 30 ng√†y\n",
        "    ax.set_xticks(days)\n",
        "    ax.grid(True, linestyle=':', alpha=0.6)\n",
        "\n",
        "    # Th√™m v√πng b√≥ng ƒë·ªï cho m√¥ h√¨nh t·ªët nh·∫•t (gi·∫£ s·ª≠ l√† Random Forest ho·∫∑c XGBoost)\n",
        "    # T√¨m m√¥ h√¨nh c√≥ sai s·ªë th·∫•p nh·∫•t ƒë·ªÉ highlight\n",
        "    best_model = min(results, key=lambda x: results[x]['Total_Error_Pct'])\n",
        "    ax.fill_between(days, actual_30d.values, results[best_model]['Forecasts'],\n",
        "                    color='gray', alpha=0.1, label=f'Kho·∫£ng sai l·ªách {best_model}')\n",
        "\n",
        "    ax.legend(loc='upper right', frameon=True, shadow=True, fontsize=10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    # Call visualization functions\n",
        "visualize_comparison(results, actual_30d)\n",
        "visualize_optimization_history(studies_dict)\n",
        "visualize_forecasts(results, actual_30d)"
      ],
      "metadata": {
        "id": "a58Y89xamBA_"
      },
      "id": "a58Y89xamBA_",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}